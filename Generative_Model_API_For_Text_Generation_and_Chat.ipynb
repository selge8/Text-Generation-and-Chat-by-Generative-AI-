{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLga5cNO7QYX",
        "outputId": "1394488e-e56d-4b6c-815b-8cc898a00350",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.2/46.2 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ratelimit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-server 1.24.0 requires anyio<4,>=3.1.0, but you have anyio 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymupdf\n",
            "  Downloading pymupdf-1.25.4-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.25.4-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.25.4\n"
          ]
        }
      ],
      "source": [
        "!pip install -q gradio\n",
        "!pip install -q ratelimit backoff\n",
        "!pip install -q PyPDF2\n",
        "!pip install -q -U google-genai\n",
        "!pip install -q pypdf pandas\n",
        "!pip install pymupdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUY6ns43FHMp"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import google.generativeai as genai\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "from PyPDF2 import PdfReader\n",
        "from google.api_core.exceptions import TooManyRequests\n",
        "from ratelimit import limits, RateLimitException\n",
        "from backoff import on_exception, expo\n",
        "from google.colab import userdata\n",
        "import time\n",
        "import fitz\n",
        "\n",
        "# API Configuration\n",
        "GOOGLE_API_KEY = userdata.get(\"googleapi\")\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Setup for logging\n",
        "logging.basicConfig(\n",
        "    filename=\"api_errors.log\", level=logging.ERROR, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
        ")\n",
        "\n",
        "# Token and Rate Limits\n",
        "MAX_TOKENS = 1_000_000         # 1 million tokens for the context window\n",
        "TOKEN_WARNING_THRESHOLD = 0.8  # Warn at 80% of token limit\n",
        "MAX_RPM = 15                   # Requests per minute\n",
        "MAX_RPD = 1500                 # Requests per day\n",
        "MAX_RETRIES=3\n",
        "\n",
        "# Track global token usage and request counts\n",
        "tokens_used = 0\n",
        "requests_made_today = 0\n",
        "tokens_used_this_minute = 0\n",
        "last_request_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2M4RXcgyFPLi"
      },
      "outputs": [],
      "source": [
        "def retry_on_rate_limit_error(func): # Retries a function call on rate limit error with a 60s wait time.\n",
        "\n",
        "    def wrapper(*args, **kwargs):\n",
        "        retries = 3  # Max retries for rate limit errors\n",
        "        for attempt in range(retries):\n",
        "            try:\n",
        "                return func(*args, **kwargs)\n",
        "            except RateLimitException as e:\n",
        "                print(f\"âš ï¸ Rate limit exceeded. Retrying after 60 seconds... (Attempt {attempt + 1})\")\n",
        "                time.sleep(60)  # Wait for 60 seconds before retrying\n",
        "                continue\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ Error: {e}\")\n",
        "                return None\n",
        "        return \"âŒ Error: Max retry attempts reached.\"\n",
        "    return wrapper\n",
        "\n",
        "@retry_on_rate_limit_error\n",
        "@on_exception(expo, RateLimitException, max_tries=3)\n",
        "@limits(calls=MAX_RPM, period=60)\n",
        "def call_api(prompt, chat=None, text_mode=True): # Sends API requests for text or chat mode\n",
        "    try:\n",
        "        if text_mode:\n",
        "            return text_generation_with_token_tracking(prompt)\n",
        "        else:\n",
        "            return chat_mode_with_history_tracking(prompt, chat)\n",
        "    except RateLimitException as e:\n",
        "        print(f\"âš ï¸ Rate limit exceeded. Retrying after 60 seconds... {str(e)}\")\n",
        "        time.sleep(60)  # Wait for 60 seconds before retrying\n",
        "        return call_api(prompt, chat, text_mode)  # Retry after waiting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5cmuVQoFWyY"
      },
      "outputs": [],
      "source": [
        "def extract_text_from_files(files): # Extracts text from multiple txt, csv, and pdf files.\n",
        "\n",
        "    extracted_texts = []\n",
        "\n",
        "    for file in files:\n",
        "        try:\n",
        "            if file.name.endswith(\".txt\"):\n",
        "                extracted_texts.append(file.read().decode(\"utf-8\"))\n",
        "            elif file.name.endswith(\".csv\"):\n",
        "                df = pd.read_csv(file)\n",
        "                extracted_texts.append(df.to_string())\n",
        "            elif file.name.endswith(\".pdf\"):\n",
        "                with fitz.open(file.name) as doc:\n",
        "                    text = \"\"\n",
        "                    for page in doc:\n",
        "                        text += page.get_text()\n",
        "                    extracted_texts.append(text)\n",
        "            else:\n",
        "                extracted_texts.append(f\"Unsupported file format: {file.name}\")\n",
        "        except Exception as e:\n",
        "            extracted_texts.append(f\"Error reading file {file.name}: {str(e)}\")\n",
        "\n",
        "    return \"\\n\\n\".join(extracted_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEhx701RFbgd"
      },
      "outputs": [],
      "source": [
        "def text_generation_with_token_tracking(prompt): # Generates text while tracking token usage,  Issues a warning if the prompt is nearing the context window limit.\n",
        "\n",
        "    global tokens_used\n",
        "\n",
        "    # Estimate tokens for the prompt\n",
        "    prompt_tokens = len(prompt.split())\n",
        "\n",
        "    # Check if adding this prompt would exceed the total allowed tokens\n",
        "    if tokens_used + prompt_tokens > MAX_TOKENS:\n",
        "        warning = f\"âŒ Error: Token limit exceeded! (Used: {tokens_used}/{MAX_TOKENS})\"\n",
        "        print(warning)\n",
        "        return warning\n",
        "\n",
        "    # Warn if approaching threshold\n",
        "    if tokens_used + prompt_tokens > TOKEN_WARNING_THRESHOLD * MAX_TOKENS:\n",
        "        print(f\"âš ï¸ Warning: Token usage is nearing the limit ({tokens_used + prompt_tokens}/{MAX_TOKENS}).\")\n",
        "\n",
        "    # Add prompt tokens to total usage\n",
        "    tokens_used += prompt_tokens\n",
        "\n",
        "    model = genai.GenerativeModel(\"gemini-1.5-flash-latest\")  # Create the model instance\n",
        "\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        if response is None or not hasattr(response, \"text\"):\n",
        "            error = \"âŒ Error: No response from model.\"\n",
        "            print(error)\n",
        "            return error\n",
        "\n",
        "        # Estimate tokens in the response and add them to usage\n",
        "        response_tokens = len(response.text.split())\n",
        "        tokens_used += response_tokens\n",
        "\n",
        "        # Warn if after adding response tokens, we're near the limit\n",
        "        if tokens_used > TOKEN_WARNING_THRESHOLD * MAX_TOKENS:\n",
        "            print(f\"âš ï¸ Warning: After generation, token usage is high: {tokens_used}/{MAX_TOKENS} tokens used.\")\n",
        "\n",
        "        # Print token usage for debugging\n",
        "        print(f\"ğŸ“Š Text Generation Token Usage: {tokens_used}/{MAX_TOKENS} tokens used.\")\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        error_msg = f\"âŒ Error during text generation: {e}\"\n",
        "        print(error_msg)\n",
        "        return error_msg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-UQAEcs7pv9L"
      },
      "outputs": [],
      "source": [
        "def chat_mode_with_history_tracking(prompt, chat): # It calculates tokens from both the history and the new prompt, warning when the context is close to its limit.\n",
        "\n",
        "    global tokens_used\n",
        "\n",
        "    # Calculate tokens used in the conversation history\n",
        "    history_tokens = 0\n",
        "    for message in chat.history:\n",
        "        history_tokens += len(message.parts[0].text.split())\n",
        "\n",
        "    prompt_tokens = len(prompt.split())\n",
        "    total_new_tokens = history_tokens + prompt_tokens\n",
        "\n",
        "    # Check if this chat request will exceed the token limit.\n",
        "    if tokens_used + total_new_tokens > MAX_TOKENS:\n",
        "        warning = f\"âŒ Error: Token limit exceeded in chat mode! (Used: {tokens_used}/{MAX_TOKENS})\"\n",
        "        print(warning)\n",
        "        return warning\n",
        "\n",
        "    # Warn if approaching threshold\n",
        "    if tokens_used + total_new_tokens > TOKEN_WARNING_THRESHOLD * MAX_TOKENS:\n",
        "        print(f\"âš ï¸ Warning: Chat context token usage is nearing limit ({tokens_used + total_new_tokens}/{MAX_TOKENS}).\")\n",
        "\n",
        "    # Update global token usage with current conversation tokens\n",
        "    tokens_used += total_new_tokens\n",
        "\n",
        "    # Call the Gemini chat API with the current prompt and history\n",
        "    try:\n",
        "        response = chat.send_message(prompt)\n",
        "        if response is None or not hasattr(response, \"text\"):\n",
        "            error = \"âŒ Error: No response from model in chat mode.\"\n",
        "            print(error)\n",
        "            return error\n",
        "\n",
        "        # Count tokens in the response and update\n",
        "        response_tokens = len(response.text.split())\n",
        "        tokens_used += response_tokens\n",
        "\n",
        "        # Warn if token usage becomes too high after the response\n",
        "        if tokens_used > TOKEN_WARNING_THRESHOLD * MAX_TOKENS:\n",
        "            print(f\"âš ï¸ Warning: After chat response, token usage is high: {tokens_used}/{MAX_TOKENS} tokens used.\")\n",
        "\n",
        "        print(f\"ğŸ“Š Chat Mode Token Usage: {tokens_used}/{MAX_TOKENS} tokens used.\")\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        error_msg = f\"âŒ Error during chat mode: {e}\"\n",
        "        print(error_msg)\n",
        "        return error_msg\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 719
        },
        "id": "oZIsZH6GHVHt",
        "outputId": "ede86a14-6671-443a-a42a-538455d0d0f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://56e1f36ba20d0c4c6a.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://56e1f36ba20d0c4c6a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Š Chat Mode Token Usage: 1549/1000000 tokens used.\n",
            "ğŸ“Š Chat Mode Token Usage: 4647/1000000 tokens used.\n",
            "ğŸ“Š Chat Mode Token Usage: 9326/1000000 tokens used.\n",
            "ğŸ“Š Chat Mode Token Usage: 15667/1000000 tokens used.\n",
            "ğŸ“Š Chat Mode Token Usage: 23603/1000000 tokens used.\n",
            "ğŸ“Š Chat Mode Token Usage: 33131/1000000 tokens used.\n",
            "ğŸ“Š Chat Mode Token Usage: 44242/1000000 tokens used.\n"
          ]
        }
      ],
      "source": [
        "def chat_interface(prompt, files, mode): # Processes input, files, and chooses text/chat mode\n",
        "\n",
        "    file_text = \"\"\n",
        "\n",
        "    # Process files if any are uploaded\n",
        "    if files:\n",
        "        file_text = extract_text_from_files(files)\n",
        "\n",
        "    combined_prompt = f\"{prompt}\\n\\n[File Content]:\\n{file_text}\" if file_text else prompt\n",
        "\n",
        "  # Call the appropriate function based on selected mode\n",
        "    if mode == \"Text Generation\":\n",
        "        return call_api(combined_prompt, text_mode=True)  #  Uses call_api for proper handling\n",
        "    else:  # Chat Mode\n",
        "        return call_api(combined_prompt, chat, text_mode=False)  #  Ensures correct chat API call\n",
        "\n",
        "def build_chatbot(system_instruction): #  Defines a function to create a chatbot with instructions\n",
        "\n",
        "    model = genai.GenerativeModel(\"gemini-1.5-flash-latest\", system_instruction=system_instruction)\n",
        "    chat = model.start_chat(history=[])\n",
        "    return chat\n",
        "\n",
        "# System prompt\n",
        "system_prompt = \"\"\"You are an attentive and supportive academic assistant.\n",
        "Your task is to provide assistance.\n",
        "I will provide you the question.\n",
        "\n",
        "If the answer cannot be found,\n",
        "kindly respond with 'I don't know'.\n",
        "\n",
        "After answering each question, please provide a detailed\n",
        "explanation, breaking down the answer step by step.\n",
        "\n",
        "If you are ready, I will provide you the question.\n",
        "\"\"\"\n",
        "\n",
        "# Create a chatbot instance using a system prompt\n",
        "chat = build_chatbot(system_prompt)\n",
        "\n",
        "# Updated Gradio UI with Mode Selector\n",
        "demo = gr.Interface(\n",
        "    fn=chat_interface,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Prompt\", value=\"\"),\n",
        "        gr.Files(label=\"Upload files (optional)\", type=\"filepath\"),\n",
        "        gr.Radio([\"Text Generation\", \"Chat Mode\"], label=\"Mode\", value=\"Chat Mode\")  # Default: Chat Mode\n",
        "    ],\n",
        "    outputs=\"markdown\",\n",
        "    title=\"Chat with Gemini\",\n",
        "    description=\"Choose between Text Generation or Chat Mode. Type your question and optionally upload files.\"\n",
        ")\n",
        "\n",
        "# Launch UI\n",
        "demo.launch(share=True, debug=True)\n",
        "\n",
        "# Debugging token usage\n",
        "print(f\"Total tokens used: {tokens_used}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}